"""
Implementation of von Mises distribution sampling using the Best-Fisher algorithm.
Based on: Best & Fisher (1979) "Efficient Simulation of the von Mises Distribution"
"""

import jax
import jax.numpy as jnp
from jax import lax, random
import jax.scipy.special as jsp
from typing import Tuple, Union, Optional, Any, Callable

Array = Any  # JAX Array type alias

def vmises_log_prob(x: Array, loc: Array, concentration: Array) -> Array:
    """
    Compute the log probability density of the von Mises distribution.
    
    Args:
        x: Value to compute log probability for, in radians
        loc: Mean direction parameter (μ)
        concentration: Concentration parameter (κ)
        
    Returns:
        log probability density values
    """
    # Log normalization constant: log(1/(2π*I₀(κ)))
    # Use log(I₀(κ)) = bessel_i0e(κ) + κ to ensure numerical stability
    log_i0 = jnp.log(jsp.i0e(concentration)) + concentration
    log_norm = -jnp.log(2 * jnp.pi) - log_i0
    
    # Log density: log_norm + κ*cos(x - μ)
    return log_norm + concentration * jnp.cos(x - loc)


def vmises_entropy(concentration: Array) -> Array:
    """
    Compute the entropy of the von Mises distribution.
    
    The entropy formula for von Mises distribution is:
    
    H(κ) = -κ * I₁(κ)/I₀(κ) + log(2π * I₀(κ))
    
    Using exponentially scaled Bessel functions for numerical stability:
    H(κ) = -κ * i1e(κ)/i0e(κ) + log(2π * i0e(κ)) + κ
    
    Args:
        concentration: Concentration parameter (κ)
        
    Returns:
        Entropy value of the von Mises distribution
    """
    # Ensure concentration is an array for consistent handling
    concentration = jnp.asarray(concentration)
    
    # Compute ratio of Bessel functions using exponentially scaled versions
    bessel_ratio = jsp.i1e(concentration) / jsp.i0e(concentration)
    
    # Compute entropy using the formula from SciPy implementation
    # H(κ) = -κ * i1e(κ)/i0e(κ) + log(2π * i0e(κ)) + κ
    return (-concentration * bessel_ratio + 
            jnp.log(2 * jnp.pi * jsp.i0e(concentration)) + 
            concentration)


def compute_p(kappa: Array) -> Array:
    """
    Compute optimal p parameter for the wrapped Cauchy envelope.
    
    p = (τ-(2τ)^(1/2))/2κ, where τ = 1+(1+4κ^2)^(1/2)
    
    Args:
        kappa: Concentration parameter (κ)
        
    Returns:
        Optimal p value for the wrapped Cauchy envelope
    """
    # Exact implementation from Best & Fisher paper
    # For very small kappa values, p should approach 0.5
    kappa = jnp.asarray(kappa)
    
    # Original formula: p = κ/(2 + sqrt(4 + κ²)) + sqrt(1 + κ²/(4 + κ²))
    # Simplified to avoid cancellation errors
    tau = 1.0 + jnp.sqrt(1.0 + 4.0 * kappa * kappa)
    p = (tau - jnp.sqrt(2.0 * tau)) / (2.0 * kappa)
    
    # In the limit as kappa → 0, p → 0.5, and as kappa → ∞, p → 0
    # For numerical stability, ensure p is in valid range
    p = jnp.clip(p, 0.0, 0.5)
    
    # For very small kappa, we need to handle the limit case
    p = jnp.where(kappa < 1e-6, 0.5, p)
    
    return p


def sample_von_mises(
    key: Array,
    loc: Array,
    concentration: Array,
    shape: Optional[Tuple[int, ...]] = None,
    max_iters: int = 100,
) -> Array:
    """
    Sample from von Mises distribution using Best-Fisher algorithm.
    
    This implementation is designed to be compatible with jax.pmap and
    to handle parameters generated by neural networks.
    
    Args:
        key: PRNG key
        loc: Mean direction parameter (μ), in radians
        concentration: Concentration parameter (κ) > 0
        shape: Output shape (optional, inferred from inputs if None)
        max_iters: Maximum number of rejection sampling iterations
        
    Returns:
        Samples from von Mises(loc, concentration), in radians in range [-π, π]
    """
    # Ensure concentration is positive and inputs are JAX arrays
    concentration = jnp.asarray(concentration)
    concentration = jnp.maximum(concentration, 1e-6)
    loc = jnp.asarray(loc)
    
    # Handle batch dimensions for parameters from NN
    batched_kappa = concentration.ndim > 0
    batched_loc = loc.ndim > 0
    
    # Infer shape from the inputs if not provided
    if shape is None:
        if batched_kappa:
            output_shape = concentration.shape
        elif batched_loc:
            output_shape = loc.shape
        else:
            output_shape = ()
    else:
        # Use the provided shape
        output_shape = shape
    
    # Handle scalar inputs with batched outputs
    if not batched_kappa:
        concentration = jnp.full(output_shape, concentration)
    if not batched_loc:
        loc = jnp.full(output_shape, loc)
    
    # Reshape for flat processing
    batch_shape = concentration.shape
    flat_concentration = concentration.reshape(-1)
    flat_loc = loc.reshape(-1)
    n_samples = flat_concentration.shape[0]
    
    # For extreme concentrations, use direct sampling methods
    # For very small concentration (close to uniform), sample from uniform
    is_small_conc = flat_concentration < 1e-4
    uniform_samples = random.uniform(
        key, shape=(n_samples,), minval=-jnp.pi, maxval=jnp.pi
    )
    
    # For very large concentration (close to normal), sample from normal
    is_large_conc = flat_concentration > 100.0
    normal_samples = random.normal(key, shape=(n_samples,))
    normal_samples = normal_samples / jnp.sqrt(flat_concentration) + flat_loc
    normal_samples = ((normal_samples + jnp.pi) % (2 * jnp.pi)) - jnp.pi
    
    # Define the sample generation for a single batch
    def generate_sample(carry: Tuple[Array, Array, Array]) -> Tuple[Array, Array, Array]:
        subkey, accepted, theta = carry
        
        # Generate 3 uniform random numbers
        subkey, sample_key = random.split(subkey)
        u = random.uniform(sample_key, shape=(3, n_samples))
        
        # Algorithm step 0: Set r = (1+p^2)/(2p)
        p = compute_p(flat_concentration)
        r = (1.0 + p**2) / (2.0 * p)
        
        # Algorithm step 1
        z = jnp.cos(jnp.pi * u[0])
        f = (1.0 + r*z) / (r + z)
        c = flat_concentration * (r - f)
        
        # Algorithm steps 2-3 (rejection test)
        accept1 = (c * (2.0 - c) - u[1]) > 0
        accept2 = accept1 | ((jnp.log(c / jnp.maximum(u[1], 1e-6)) + 1.0 - c) >= 0)
        
        # If accepted, compute theta; otherwise, keep previous value
        theta_new = jnp.sign(u[2] - 0.5) * jnp.arccos(f)
        theta = jnp.where(accept2, theta_new, theta)
        accepted = accepted | accept2
        
        return (subkey, accepted, theta)
    
    # Initialize
    init_key, subkey = random.split(key)
    init_theta = jnp.zeros(n_samples)
    init_accepted = jnp.zeros(n_samples, dtype=bool)
    
    # Run the sampling loop with a fixed number of iterations
    # to ensure compatibility with JAX transformations
    carry = (subkey, init_accepted, init_theta)
    for _ in range(max_iters):
        carry = lax.cond(
            jnp.any(~carry[1]),  # Continue if any samples are not accepted
            generate_sample,
            lambda x: x,
            carry
        )
    _, _, samples = carry
    
    # Shift by the mean direction
    samples = samples + flat_loc
    
    # Ensure samples are in [-π, π]
    samples = ((samples + jnp.pi) % (2 * jnp.pi)) - jnp.pi
    
    # Use direct sampling methods for extreme concentrations
    samples = jnp.where(is_small_conc, uniform_samples, samples)
    samples = jnp.where(is_large_conc, normal_samples, samples)
    
    # Reshape to original batch dimensions
    return samples.reshape(batch_shape) 